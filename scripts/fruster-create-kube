#!/usr/bin/env bash
# 
# Fruster Create Kube Script
# Version 0.0.1
# 
# See usage for 
# This script will bootstrap a Kubernetes on AWS
# using kops do to the heavy lifting

# Exit on error. Append "|| true" if you expect an error.
set -o errexit
# Exit on error inside any functions or subshells.
set -o errtrace
# Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
set -o nounset
# Catch the error in case mysqldump fails (but gzip succeeds) in `mysqldump |gzip`
set -o pipefail

DEBUG=0

source ./shared/utils

usage () {
	cat << EOF
Usage: $0 -option [CLUSTER NAME]

Creates a (opiniated) Kubernetes cluster on AWS. 
Cluster is created with private topololy with a bastion host to access it.

This script is a thin layer on top of Kops which does the heavy lifting.

OPTIONS:
  -k          public ssh key used to access nodes, a new one will be created if none set
  -z          aws availability zone where cluster is created, defaults to "eu-west-1a"
  -v          verbose logging
  -h 					show this message

The following env vars are supported for further customization:

FRUSTER_KOPS_STATE_STORE   set name of S3 state store bucket, will use name of cluster and suffix "-kops-state-store" if none is provided
FRUSTER_AWS_TENANCY        tenancy type of master and worker nodes, defaults to "default" which indicates shared tenancy
FRUSTER_AWS_NODE_SIZE      aws node instance type of worker nodes, defaults to "m3.medium"
FRUSTER_ADMIN_CIDR         cidr block where admin access is allowed

EXAMPLES:

# Create new cluster with default options
$0 my-new-cluster

# Set admin access cidr
export FRUSTER_ADMIN_CIDR=test
$0 my-new-cluster

EOF
	exit
}

validateDeps() {
	checkBinary aws "https://docs.aws.amazon.com/cli/latest/userguide/installing.html"
	checkBinary kubectl "https://kubernetes.io/docs/tasks/tools/install-kubectl/"
	checkBinary kops "https://github.com/kubernetes/kops/blob/master/docs/install.md"
	checkBinary jq "https://stedolan.github.io/jq/download/"
	log_success "Binary dependency checks passed"
}

validateSshKey() {
  SSH_KEY=${SSH_KEY:-}

  if [ ! -z $SSH_KEY ]; then
    # Use provided SSH key
    if [ ! -f "${SSH_KEY}" ]; then
      log_error "SSH key '${SSH_KEY}' does not exist"
      exit 1
    else
      log_debug "Using provided SSH key ${SSH_KEY}"
    fi
  else
    # SSH key flag was not passed, create path based on cluster name
    SSH_KEY_PRIV=~/.ssh/${KOPS_CLUSTER_NAME}.key
        
    # Check if key already exist at that location, if so prompt user
    if [ -f "${SSH_KEY_PRIV}" ] ; then
      if prompt "Key ${SSH_KEY_PRIV} already exists, is this the correct key pair to use"; then
        exit 1
      fi
    else 
      ssh-keygen -t rsa -b 4096 -C "`whoami`@frost.se" -N "" -f $SSH_KEY_PRIV      
    fi 

    SSH_KEY="${SSH_KEY_PRIV}.pub"
  fi

 	log_success "Using ssh key $SSH_KEY"
}


createStateStore() {
	log_info "Creating state store bucket ${KOPS_STATE_STORE_NAME}..."

	aws s3api create-bucket --bucket ${KOPS_STATE_STORE_NAME} --region us-east-1 &> /dev/null || (log_error "Failed creating state store bucket '${KOPS_STATE_STORE_NAME}', probably because bucket already exist or due to invalid permissions" && exit 1)
	aws s3api put-bucket-versioning --bucket ${KOPS_STATE_STORE_NAME}  --versioning-configuration Status=Enabled
	
	log_success "State store bucket ${KOPS_STATE_STORE_NAME} created"
}

kopsCreateCluster() {
	log_info "Creating cluster resources using kops..."	

	kops create cluster \
	    --ssh-public-key ${SSH_KEY} \
	    --topology=private \
	    --networking=weave \
	    --zones=${AWS_ZONE} \
      --node-count=${NODE_COUNT} \
	  	--node-size=${AWS_NODE_SIZE} \
	  	--master-tenancy=${AWS_TENANCY} \
	  	--node-tenancy=${AWS_TENANCY} \
	  	--admin-access=${ADMIN_CIDR} \
	  	--state=s3://${KOPS_STATE_STORE_NAME} \
	    --yes \
	    $KOPS_CLUSTER_NAME

	log_success "Finished creating kops cluster"
}

setVars() {
  if [ -z "$1" ]
  then
    log_error "Missing cluster name"
    exit 1
  fi

	if [[ "$1" != *.k8s.local ]]
	then
		log_info "Will change cluster name to ${1}.k8s.local (suffix .k8s.local is required for kops to create a cluster with Gossip enabled)"
	    KOPS_CLUSTER_NAME=${1}.k8s.local	    
	else
	    KOPS_CLUSTER_NAME=${1}	    
	fi

	if [ -z "${FRUSTER_KOPS_STATE_STORE:-}" ]; then
		KOPS_STATE_STORE_NAME="${KOPS_CLUSTER_NAME//\./-}-kops-state-store"
	else
		KOPS_STATE_STORE_NAME="${FRUSTER_KOPS_STATE_STORE}"
	fi

	AWS_NODE_SIZE=${FRUSTER_AWS_NODE_SIZE:-m3.medium}
	AWS_TENANCY=${FRUSTER_AWS_TENANCY:-default}
	ADMIN_CIDR=${FRUSTER_ADMIN_CIDR:-0.0.0.0/0}
  AWS_ZONE=${AWS_ZONE:-eu-west-1a}
	NODE_COUNT=${NODE_COUNT:-2}


	cat << EOF
About to create cluster with following properties:

Cluster name:       ${KOPS_CLUSTER_NAME}
State store:        ${KOPS_STATE_STORE_NAME}
Node size:          ${AWS_NODE_SIZE}
Node count:         ${NODE_COUNT}
Tenancy:            ${AWS_TENANCY} 
Admin cidr:         ${ADMIN_CIDR} 
AWS zone:           ${AWS_ZONE} 
SSH key:            ${SSH_KEY:-None is set, will be created}
EOF

	read -p "Does this seem right (y/n)? " choice
	case "$choice" in 
	  y|Y ) ;;
	  n|N ) exit 0;;
	  * ) { echo "Invalid choice"; exit 1; };;
	esac
}

createBastion() {
  log_info "Creating bastion host..."
  log_debug "Note that this should be done by setting --bastion flag in during kops cluster creation but due to this issue https://github.com/kubernetes/kops/issues/2881 we cannot do that at the moment"
  
  kops create instancegroup bastions \
    --role Bastion \
    --subnet utility-${AWS_ZONE} \
    --edit=false \
    --name ${KOPS_CLUSTER_NAME} \
    --state s3://${KOPS_STATE_STORE_NAME}

  kops update cluster ${KOPS_CLUSTER_NAME} \
    --state s3://${KOPS_STATE_STORE_NAME} \
    --yes
 
  log_success "Bastion was created"    
}

showPostCreateInfo() {
  cat << EOF
Congrat, cluster was successfully created.

Here are some commands that may come in handy:

# Validate cluster, you should run this now to see that node eventually will get in ready state
kops validate cluster --state s3://${KOPS_STATE_STORE_NAME}

# Delete cluster (this will undo everything you just created but not remove SSH key)
kops delete cluster ${KOPS_CLUSTER_NAME} --state s3://${KOPS_STATE_STORE_NAME} --yes

# Connect to bastion
ssh -i ${SSH_KEY_PRIV:-your-private-key} admin@$(aws elb --output=json describe-load-balancers |jq -r .LoadBalancerDescriptions[].DNSName |grep bastion-${KOPS_STATE_STORE_NAME:0:15})

IMPORTANT! Your cluster is probably not yet initialized by now, please run the validate cluster command
a couple of times until cluster and all nodes are in ready state

EOF
}

while getopts ":fvzhk:c:" opt; do
	case $opt in    	
    k)
		SSH_KEY="$OPTARG"
		;;
		c)
    NODE_COUNT="$OPTARG"
		;;
		z)
		AWS_ZONE="$OPTARG"
		;;		
    v)
    DEBUG=1
    set -o xtrace
    ;;
    h)
		usage
    ;;
    \?)
    echo "Invalid option: -$OPTARG" >&2      
    ;;    
	esac
	shift $(expr $OPTIND - 1 )
done

setVars ${1:-""}

validateDeps
validateAws
validateSshKey

createStateStore
kopsCreateCluster
createBastion

showPostCreateInfo